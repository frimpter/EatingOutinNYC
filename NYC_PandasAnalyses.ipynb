{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# LOAD THE OPEN NYC MAIN FILE #\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_file = \"/Users/frimpter/Documents/data_science/ru_datascience_bootcamp/Project 2/Data/DOHMH_New_York_City_Restaurant_Inspection_Results.csv\"\n",
    "\n",
    "nyc_df = pd.read_csv(nyc_file, encoding=\"utf-8\")\n",
    "\n",
    "# MAKE AN ADDRESS COLUMN\n",
    "nyc_df[\"ADDRESS\"] = nyc_df[\"BUILDING\"] + \" \" + nyc_df[\"STREET\"]\n",
    "\n",
    "#nyc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up column names\n",
    "\n",
    "nyc_df.columns = nyc_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "#nyc_df.head()\n",
    "\n",
    "# Review violation descriptions\n",
    "#nyc_df[\"violation_description\"].unique()\n",
    "#nyc_df[\"violation_description\"].value_counts()\n",
    "#nyc_df[\"violation_code\"].value_counts()\n",
    "\n",
    "# Find instances where violation contains \"mice\" or \"rats\" or \"roach\" or \"flies\" or \"sanitiz\" or \"hand\"\n",
    "nyc_df[\"mice\"] = nyc_df['violation_description'].str.contains('mice')\n",
    "nyc_df[\"rats\"] = nyc_df['violation_description'].str.contains('rats')\n",
    "nyc_df[\"roach\"] = nyc_df['violation_description'].str.contains('roach')\n",
    "nyc_df[\"flies\"] = nyc_df['violation_description'].str.contains('flies')\n",
    "nyc_df[\"hand\"] = nyc_df['violation_description'].str.contains('hand')\n",
    "nyc_df[\"sanitiz\"] = nyc_df['violation_description'].str.contains('sanitiz')\n",
    "\n",
    "# nyc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df = nyc_df.replace({True:1, False:0}, regex=True)\n",
    "# nyc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# CREATE THE SIDEWALK MAP DATA FILE #\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONDENSE THE NYC FILE JUST FOR WHAT WE NEED\n",
    "\n",
    "nyc_brief = nyc_df[[\"dba\", \"address\", \"cuisine_description\", \"grade\", \"grade_date\"]]\n",
    "\n",
    "# nyc_brief.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AND TRIM THE OPEN NYC SIDEWALK LICENSE FILE\n",
    "\n",
    "sidewalk_file = \"/Users/frimpter/Documents/data_science/ru_datascience_bootcamp/Project 2/Data/Sidewalk_Caf_Licenses_and_Applications.csv\"\n",
    "\n",
    "sidewalk_df = pd.read_csv(sidewalk_file, encoding=\"utf-8\")\n",
    "\n",
    "# ONLY KEEP THE COLUMNS WE REALLY NEED, MAKE AN ADDRESS COLUMN\n",
    "sidewalk_df[\"ADDRESS\"] = sidewalk_df[\"BUILDING\"] + \" \" + sidewalk_df[\"STREET\"]\n",
    "\n",
    "sidewalk_df = sidewalk_df[[\"BUSINESS_NAME\", \"ADDRESS\", \"CITY\", \"ZIP\", \"LATITUDE\", \"LONGITUDE\", \"LIC_STATUS\", \"SWC_TYPE\"]]\n",
    "sidewalk_df.columns = sidewalk_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "# sidewalk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A CONDENSED JSON FILE TO MAP SIDEWALK RESTAURANTS\n",
    "\n",
    "map_df = pd.merge(nyc_brief, sidewalk_df, on=\"address\", how=\"inner\")\n",
    "\n",
    "# MANAGE NaNs\n",
    "map_df = map_df.dropna(axis=0, how=\"any\")\n",
    "#map_df.shape\n",
    "\n",
    "# SORT BY MOST RECENT GRADE DATE\n",
    "map_df[\"grade_date\"] = pd.to_datetime(map_df[\"grade_date\"])\n",
    "#map_df.dtypes\n",
    "map_df = map_df.sort_values(\"grade_date\", ascending=False)\n",
    "\n",
    "# KEEP ONLY MOST RECENT GRADE DATE\n",
    "map_df = map_df.drop_duplicates(subset=\"dba\", keep='first')\n",
    "\n",
    "# REMOVE INACTIVE SIDEWALK SEATING LICENSES\n",
    "map_df = map_df[map_df.lic_status != \"Inactive\"]\n",
    "\n",
    "#CLEAN UP THE CUISINE DESCRIPTIONS\n",
    "map_df = map_df.replace({\"CafÃ©/Coffee/Tea\":\"Cafe/Coffee/Tea\", \"Latin \\(Cuban, Dominican, Puerto Rican, South \\& Central American\\)\":\"Latin\"}, regex=True)\n",
    "\n",
    "# RENAME COLUMNS FOR JSON\n",
    "map_df = map_df.rename(columns={\"cuisine_description\":\"cuisine\", \"grade_date\":\"date\"}).reset_index()\n",
    "\n",
    "# map_df.head()\n",
    "\n",
    "# SMALL FILE - MAKE A CSV TO INSPECT MANUALLY\n",
    "#map_df.to_csv(\"SIDEWALK_MAP.csv\", index=False, header=True)\n",
    "\n",
    "# EXPORT JSON FILE\n",
    "# map_df.to_json(\"SIDEWALK.json\", orient=\"records\", date_format=\"iso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# SIDEWALK VIOLATIONS CHART DATA #\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df = pd.merge(nyc_df, sidewalk_df, on=\"address\", how=\"inner\")\n",
    "\n",
    "# MANAGE NaNs\n",
    "chart_df = chart_df.dropna(axis=0, how=\"any\")\n",
    "#chart_df.shape\n",
    "\n",
    "# REMOVE INACTIVE SIDEWALK SEATING LICENSES\n",
    "chart_df = chart_df[chart_df.lic_status != \"Inactive\"]\n",
    "\n",
    "#CLEAN UP THE CUISINE DESCRIPTIONS\n",
    "chart_df = chart_df.replace({\"CafÃ©/Coffee/Tea\":\"Cafe/Coffee/Tea\", \"Latin \\(Cuban, Dominican, Puerto Rican, South \\& Central American\\)\":\"Latin\"}, regex=True)\n",
    "\n",
    "# MATCH THE CATEGORIES TO THOSE IN THE MAP (AS CODED IN map.js)\n",
    "chart_df[\"cuisine\"] = chart_df[\"cuisine_description\"]\n",
    "chart_df[\"cuisine\"] = chart_df[\"cuisine\"].replace({\"Greek\":\"Mediterranean\", \"Chinese\":\"Asian\", \"Korean\":\"Asian\", \"Japanese\":\"Asian\", \"Thai\":\"Asian\", \"Indian\":\"Asian\", \"Bakery\":\"BakeryCafe\",\"Donuts\":\"BakeryCafe\", \"Cafe/Coffee/Tea\":\"BakeryCafe\", \"Pizza\":\"Italian\", \"Pizza/Italian\":\"Italian\", \"Italian/Italian\":\"Italian\", \"Spanish\":\"Mexican\", \"Latin\":\"Mexican\"}, regex=True)\n",
    "\n",
    "# CHANGE THE REMAINING TO \"Other\"\n",
    "final_cuisines = [\"American\",\"Asian\",\"BakeryCafe\",\"French\",\"Italian\",\"Mediterranean\",\"Mexican\",\"Other\"]\n",
    "cuisines = chart_df[\"cuisine\"]\n",
    "\n",
    "for item in cuisines:\n",
    "    if item not in final_cuisines:\n",
    "        chart_df[\"cuisine\"] = chart_df[\"cuisine\"].replace({item:\"Other\"}, regex=True)\n",
    "\n",
    "# REVIEW DF\n",
    "#chart_df.shape\n",
    "#chart_df.head()\n",
    "\n",
    "# NEW DF LIMITED THE COLUMNS WE NEED\n",
    "final_chart = chart_df[[\"camis\",\"dba\",\"boro\",\"cuisine\",\"inspection_date\",\"action\",\"violation_code\",\"violation_description\",\"critical_flag\",\"score\",\"grade\",\"grade_date\",\"mice\",\"rats\",\"roach\",\"flies\",\"hand\",\"sanitiz\",\"lic_status\",\"swc_type\"]]\n",
    "\n",
    "# final_chart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT DF TO CHART REQUIREMENTS BY CUISINE TYPE\n",
    "\n",
    "# MAKE A COPY\n",
    "swv = final_chart\n",
    "# del swv[\"camis\"]\n",
    "# swv.head()\n",
    "\n",
    "# PULL OUT THE COUNTS FOR EACH CUISINE TUPE TO BECOME A TOTAL COLUMN (TO CALC PERCENTAGES)\n",
    "counts = swv.groupby(\"cuisine\", as_index=False).count()\n",
    "# counts\n",
    "# cuisine_count = counts[\"dba\"]\n",
    "\n",
    "# GROUPBY CUISINE TYPE AS A RUNNING COLUMN\n",
    "swv = swv.groupby(\"cuisine\", as_index=False).sum().round(1)\n",
    "\n",
    "swv[\"total\"] = counts[\"dba\"]\n",
    "\n",
    "swv[\"mean_score\"] = (swv[\"score\"] / swv[\"total\"]).round(1)\n",
    "swv[\"mice_pct\"] = (swv[\"mice\"] / swv[\"total\"]*100).round(1)\n",
    "swv[\"rats_pct\"] = (swv[\"rats\"] / swv[\"total\"]*100).round(1)\n",
    "swv[\"roach_pct\"] = (swv[\"roach\"] / swv[\"total\"]*100).round(1)\n",
    "swv[\"flies_pct\"] = (swv[\"flies\"] / swv[\"total\"]*100).round(1)\n",
    "swv[\"hand_pct\"] = (swv[\"hand\"] / swv[\"total\"]*100).round(1)\n",
    "swv[\"sanitiz_pct\"] = (swv[\"sanitiz\"] / swv[\"total\"]*100).round(1)\n",
    "\n",
    "# CALCULATE AN \"Other\" CATEGORY\n",
    "swv[\"subtotal\"] = swv[\"mice_pct\"] + swv[\"rats_pct\"] + swv[\"roach_pct\"] + swv[\"flies_pct\"] + swv[\"hand_pct\"] + swv[\"sanitiz_pct\"]\n",
    "swv[\"other\"] = 100 - swv[\"subtotal\"]\n",
    "\n",
    "# REVIEW\n",
    "# swv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL COPY AND CLEAN IT UP FOR EXPORT TO JSON\n",
    "\n",
    "swchart = swv\n",
    "swchart = swchart[[\"cuisine\",\"mice_pct\",\"rats_pct\",\"roach_pct\",\"flies_pct\",\"hand_pct\",\"sanitiz_pct\"]]\n",
    "swchart = swchart.rename(columns={\"mice_pct\":\"MICE\", \"rats_pct\":\"RATS\", \"roach_pct\":\"ROACHES\",\"flies_pct\":\"FLIES\",\"hand_pct\":\"HANDS\",\"sanitiz_pct\":\"SANITIZATION\"})\n",
    "\n",
    "# swchart\n",
    "# swchart.to_json(\"/Users/frimpter/Documents/data_science/ru_datascience_bootcamp/Project 2/SWCHART.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# HEATMAP FROM ALL NYC DATA #\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TRIM THE COLUMNS\n",
    "nyc_df = nyc_df[[\"camis\",\"dba\",\"boro\",\"cuisine_description\",\"inspection_date\",\"violation_code\",\"violation_description\",\"critical_flag\",\"score\",\"grade\",\"grade_date\",\"mice\",\"rats\",\"roach\",\"flies\",\"hand\",\"sanitiz\"]]\n",
    "\n",
    "# CLEAN AND ALIGN CUISINE DESCRIPTIONS\n",
    "nyc_df = nyc_df.replace({\"CafÃ©/Coffee/Tea\":\"Cafe/Coffee/Tea\", \"Latin \\(Cuban, Dominican, Puerto Rican, South \\& Central American\\)\":\"Latin\"}, regex=True)\n",
    "\n",
    "# MATCH THE CATEGORIES TO THOSE IN THE MAP (AS CODED IN map2.js)\n",
    "nyc_df[\"cuisine\"] = nyc_df[\"cuisine_description\"]\n",
    "nyc_df[\"cuisine\"] = nyc_df[\"cuisine\"].replace({\"Greek\":\"Mediterranean\", \"Chinese\":\"Asian\", \"Korean\":\"Asian\", \"Japanese\":\"Asian\", \"Thai\":\"Asian\", \"Indian\":\"Asian\", \"Bakery\":\"BakeryCafe\",\"Donuts\":\"BakeryCafe\", \"Cafe/Coffee/Tea\":\"BakeryCafe\", \"Pizza\":\"Italian\", \"Pizza/Italian\":\"Italian\", \"Italian/Italian\":\"Italian\", \"Spanish\":\"Mexican\", \"Latin\":\"Mexican\"}, regex=True)\n",
    "\n",
    "# CHANGE THE REMAINING TO \"Other\"\n",
    "final_cuisines = [\"American\",\"Asian\",\"BakeryCafe\",\"French\",\"Italian\",\"Mediterranean\",\"Mexican\",\"Other\"]\n",
    "cuisines = nyc_df[\"cuisine\"]\n",
    "\n",
    "for item in cuisines:\n",
    "    if item not in final_cuisines:\n",
    "        nyc_df[\"cuisine\"] = nyc_df[\"cuisine\"].replace({item:\"Other\"}, regex=True)\n",
    "\n",
    "del nyc_df[\"cuisine_description\"]\n",
    "\n",
    "nyc_df[\"inspection_date\"] = pd.to_datetime(nyc_df[\"inspection_date\"])\n",
    "# nyc_df.dtypes\n",
    "\n",
    "# nyc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A GROUPBY AND CLEAN IT UP\n",
    "heat_df = nyc_df.groupby([\"boro\",\"cuisine\"], as_index=False).mean().round(1)\n",
    "heat_df = heat_df[[\"boro\",\"cuisine\",\"score\"]]\n",
    "\n",
    "# Drop the Missing Boro data row\n",
    "heat_df.drop([24], axis=0, inplace=True)\n",
    "\n",
    "# There is no French Restaurant (apparently) in Staten Island in the data file, insert a \"0\" value here\n",
    "heat_df.loc[35.5] = [\"STATEN ISLAND\", \"French\", \"0\"]\n",
    "heat_df = heat_df.sort_index()\n",
    "heat_df.reset_index()\n",
    "\n",
    "# heat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PULL THE DATA TO FORMAT A DF FOR THE HEATMAP\n",
    "\n",
    "# STORE EACH SERIES\n",
    "bronx = heat_df.loc[0:7,\"score\"]\n",
    "brooklyn = heat_df.loc[8:15,\"score\"].reset_index()[[\"score\"]]\n",
    "manhattan = heat_df.loc[16:23,\"score\"].reset_index()[[\"score\"]]\n",
    "queens = heat_df.loc[24:31,\"score\"].reset_index()[[\"score\"]]\n",
    "statenisland = heat_df.loc[32:39,\"score\"].reset_index()[[\"score\"]]\n",
    "\n",
    "# LOAD EACH SERIES INTO ITS OWN COLUMN\n",
    "newheat = heat_df[[\"cuisine\"]][:8]\n",
    "newheat[\"BRONX\"] = bronx\n",
    "newheat[\"BROOKLYN\"] = brooklyn\n",
    "newheat[\"MANHATTAN\"] = manhattan\n",
    "newheat[\"QUEENS\"] = queens\n",
    "newheat[\"STATEN_ISLAND\"] = statenisland\n",
    "\n",
    "# newheat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT TO JSON SPLIT FILE\n",
    "\n",
    "# Preview the JSON formatted data\n",
    "heatlist = newheat[[\"BRONX\",\"BROOKLYN\",\"MANHATTAN\",\"QUEENS\",\"STATEN_ISLAND\"]].to_json(orient=\"split\")\n",
    "\n",
    "# EXPORT\n",
    "heatlist = newheat[[\"BRONX\",\"BROOKLYN\",\"MANHATTAN\",\"QUEENS\",\"STATEN_ISLAND\"]].to_json(\"/Users/frimpter/Documents/data_science/ru_datascience_bootcamp/Project 2/HEAT.json\", orient=\"split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# ROLLING AVERAGE SCORES #\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-RUN AND REVIEW NYC DATA \n",
    "# nyc_df.head()\n",
    "\n",
    "# MAKE A NEW, LEAN DF AND TIDY IT UP\n",
    "line_df = nyc_df[[\"cuisine\",\"inspection_date\",\"score\"]]\n",
    "line_df = line_df.dropna(axis=0, how=\"any\")\n",
    "line_df.shape # (370367, 3) >> after dropna (350330, 3)\n",
    "\n",
    "# SORT BY INSPECTION DATE\n",
    "line_df = line_df.sort_values(\"inspection_date\", ascending=True)\n",
    "\n",
    "# line_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP BY CUISINE TYPE AND INSPECTION DATE, USING MEAN SCORE FOR A DAY'S INSPECTION SCORES\n",
    "group_df = line_df.groupby([\"cuisine\", \"inspection_date\"], as_index=False).mean().round(1)\n",
    "\n",
    "# PREVIEW THE .rolling FUNCTION TO CALCULATE ROLLING AVERAGE\n",
    "# group_df[\"rolling\"] = group_df[\"score\"].rolling(5).mean().round(1)\n",
    "\n",
    "# group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD NEW COLUMNS FOR CUISINE DATE, SCORE AND ROLLING AVERAGE SCORE\n",
    "# Calculate rolling average on the fly to keep separation between cuisine types\n",
    "\n",
    "# American\n",
    "group_df[\"Am_date\"] = group_df.loc[group_df[\"cuisine\"] == \"American\", \"inspection_date\"].reset_index()[[\"inspection_date\"]]\n",
    "group_df[\"Am_score\"] = group_df.loc[group_df[\"cuisine\"] == \"American\", \"score\"].reset_index()[[\"score\"]]\n",
    "group_df[\"Am_roll\"] = group_df[\"Am_score\"].rolling(100).mean().round(1)\n",
    "\n",
    "# Asian\n",
    "group_df[\"As_date\"] = group_df.loc[group_df[\"cuisine\"] == \"Asian\", \"inspection_date\"].reset_index()[[\"inspection_date\"]]\n",
    "group_df[\"As_score\"] = group_df.loc[group_df[\"cuisine\"] == \"Asian\", \"score\"].reset_index()[[\"score\"]]\n",
    "group_df[\"As_roll\"] = group_df[\"As_score\"].rolling(100).mean().round(1)\n",
    "\n",
    "# BakeryCafe\n",
    "group_df[\"BC_date\"] = group_df.loc[group_df[\"cuisine\"] == \"BakeryCafe\", \"inspection_date\"].reset_index()[[\"inspection_date\"]]\n",
    "group_df[\"BC_score\"] = group_df.loc[group_df[\"cuisine\"] == \"BakeryCafe\", \"score\"].reset_index()[[\"score\"]]\n",
    "group_df[\"BC_roll\"] = group_df[\"BC_score\"].rolling(100).mean().round(1)\n",
    "\n",
    "# French\n",
    "group_df[\"Fr_date\"] = group_df.loc[group_df[\"cuisine\"] == \"French\", \"inspection_date\"].reset_index()[[\"inspection_date\"]]\n",
    "group_df[\"Fr_score\"] = group_df.loc[group_df[\"cuisine\"] == \"French\", \"score\"].reset_index()[[\"score\"]]\n",
    "group_df[\"Fr_roll\"] = group_df[\"Fr_score\"].rolling(100).mean().round(1)\n",
    "\n",
    "# Italian\n",
    "group_df[\"It_date\"] = group_df.loc[group_df[\"cuisine\"] == \"Italian\", \"inspection_date\"].reset_index()[[\"inspection_date\"]]\n",
    "group_df[\"It_score\"] = group_df.loc[group_df[\"cuisine\"] == \"Italian\", \"score\"].reset_index()[[\"score\"]]\n",
    "group_df[\"It_roll\"] = group_df[\"It_score\"].rolling(100).mean().round(1)\n",
    "\n",
    "# Mediterranean\n",
    "group_df[\"Me_date\"] = group_df.loc[group_df[\"cuisine\"] == \"Mediterranean\", \"inspection_date\"].reset_index()[[\"inspection_date\"]]\n",
    "group_df[\"Me_score\"] = group_df.loc[group_df[\"cuisine\"] == \"Mediterranean\", \"score\"].reset_index()[[\"score\"]]\n",
    "group_df[\"Me_roll\"] = group_df[\"Me_score\"].rolling(100).mean().round(1)\n",
    "\n",
    "# Mexican\n",
    "group_df[\"Mx_date\"] = group_df.loc[group_df[\"cuisine\"] == \"Mexican\", \"inspection_date\"].reset_index()[[\"inspection_date\"]]\n",
    "group_df[\"Mx_score\"] = group_df.loc[group_df[\"cuisine\"] == \"Mexican\", \"score\"].reset_index()[[\"score\"]]\n",
    "group_df[\"Mx_roll\"] = group_df[\"Mx_score\"].rolling(100).mean().round(1)\n",
    "\n",
    "# Other\n",
    "group_df[\"Ot_date\"] = group_df.loc[group_df[\"cuisine\"] == \"Other\", \"inspection_date\"].reset_index()[[\"inspection_date\"]]\n",
    "group_df[\"Ot_score\"] = group_df.loc[group_df[\"cuisine\"] == \"Other\", \"score\"].reset_index()[[\"score\"]]\n",
    "group_df[\"Ot_roll\"] = group_df[\"Ot_score\"].rolling(100).mean().round(1)\n",
    "\n",
    "\n",
    "# TRIM THE ORIGINAL COLUMNS OUT\n",
    "del group_df[\"cuisine\"]\n",
    "del group_df[\"inspection_date\"]\n",
    "del group_df[\"score\"]\n",
    "\n",
    "# group_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT TO JSON FILE\n",
    "group_df.to_json(\"/Users/frimpter/Documents/data_science/ru_datascience_bootcamp/Project 2/ROLLING.json\", orient=\"records\", date_format=\"iso\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
